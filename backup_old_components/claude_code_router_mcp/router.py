"""
Claude Code Router MCP - æ ¸å¿ƒè·¯ç”±å™¨
å¤šAIæ¨¡å‹æ™ºèƒ½è·¯ç”±ç³»çµ±çš„æ ¸å¿ƒå¯¦ç¾
"""

import asyncio
import json
import logging
import time
import hashlib
from typing import Dict, List, Optional, Any, AsyncGenerator
from dataclasses import asdict
import httpx
from datetime import datetime, timedelta
import random

from .models import (
    ModelProvider, RouterRequest, RouterResponse, RouterStats,
    ModelConfig, SupportedModel
)
from .config import RouterConfig, ModelConfigManager
from .utils import RouterUtils
from .cache import RouterCache
from .load_balancer import LoadBalancer


logger = logging.getLogger(__name__)


class ClaudeCodeRouterMCP:
    """Claude Code Router MCP - å¤šAIæ¨¡å‹æ™ºèƒ½è·¯ç”±ç³»çµ±"""
    
    def __init__(self, config: RouterConfig = None):
        self.config = config or RouterConfig()
        self.model_manager = ModelConfigManager()
        self.cache = RouterCache(
            ttl=self.config.cache_ttl,
            max_size=self.config.cache_max_size
        )
        self.load_balancer = LoadBalancer(self.config.load_balancing_strategy)
        self.stats = RouterStats()
        self.utils = RouterUtils()
        
        # HTTPå®¢æˆ¶ç«¯
        self.http_client = httpx.AsyncClient(
            timeout=30.0,
            limits=httpx.Limits(max_keepalive_connections=20, max_connections=100)
        )
        
        # å¥åº·æª¢æŸ¥
        self.health_status: Dict[str, Dict[str, Any]] = {}
        self.health_check_task: Optional[asyncio.Task] = None
        
        # è«‹æ±‚é™åˆ¶
        self.request_counts: Dict[str, List[float]] = {}
        
        logger.info("ğŸš€ Claude Code Router MCP åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """åˆå§‹åŒ–è·¯ç”±å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ–Claude Code Router MCP...")
        
        # é©—è­‰é…ç½®
        config_errors = self.model_manager.validate_configs()
        if config_errors:
            logger.warning(f"é…ç½®è­¦å‘Š: {config_errors}")
        
        # å•Ÿå‹•å¥åº·æª¢æŸ¥
        if self.config.enable_failover:
            self.health_check_task = asyncio.create_task(self._health_check_loop())
        
        # åˆå§‹åŒ–è² è¼‰å‡è¡¡å™¨
        enabled_models = self.model_manager.get_enabled_models()
        for model in enabled_models:
            self.load_balancer.add_endpoint(model.model_id, model.priority)
        
        logger.info("âœ… Claude Code Router MCP åˆå§‹åŒ–å®Œæˆ")
    
    async def route_request(self, request: RouterRequest) -> RouterResponse:
        """è·¯ç”±è«‹æ±‚åˆ°é©ç•¶çš„AIæ¨¡å‹"""
        start_time = time.time()
        
        try:
            # æª¢æŸ¥ç·©å­˜
            if self.config.enable_cache and not request.stream:
                cached_response = await self._get_cached_response(request)
                if cached_response:
                    logger.info(f"ğŸ¯ ç·©å­˜å‘½ä¸­: {request.model}")
                    cached_response.cached = True
                    return cached_response
            
            # é¸æ“‡æ¨¡å‹
            model_config = await self._select_model(request)
            if not model_config:
                raise Exception(f"ç„¡æ³•æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹: {request.model}")
            
            # æª¢æŸ¥é€Ÿç‡é™åˆ¶
            if not await self._check_rate_limit(model_config.model_id):
                raise Exception(f"é€Ÿç‡é™åˆ¶: {model_config.model_id}")
            
            # ç™¼é€è«‹æ±‚
            response = await self._send_request(request, model_config)
            
            # è¨ˆç®—æˆæœ¬
            cost = self._calculate_cost(response, model_config)
            response.cost = cost
            
            # ç·©å­˜éŸ¿æ‡‰
            if self.config.enable_cache and not request.stream:
                await self._cache_response(request, response)
            
            # æ›´æ–°çµ±è¨ˆ
            response_time = time.time() - start_time
            self.stats.add_request(
                model_config.model_id,
                model_config.provider,
                True,
                response_time,
                cost
            )
            
            # æ›´æ–°æ¨¡å‹æ€§èƒ½æŒ‡æ¨™
            await self._update_model_metrics(model_config.model_id, response_time, True)
            
            response.response_time = response_time
            
            logger.info(f"âœ… è·¯ç”±æˆåŠŸ: {request.model} -> {model_config.model_id} ({response_time:.2f}s, ${cost:.4f})")
            
            return response
            
        except Exception as e:
            response_time = time.time() - start_time
            logger.error(f"âŒ è·¯ç”±å¤±æ•—: {request.model} -> {str(e)}")
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats.add_request(
                request.model,
                ModelProvider.ANTHROPIC,  # é»˜èªæä¾›å•†
                False,
                response_time,
                0.0
            )
            
            raise e
    
    async def _select_model(self, request: RouterRequest) -> Optional[ModelConfig]:
        """é¸æ“‡æœ€ä½³æ¨¡å‹"""
        # é¦–å…ˆæª¢æŸ¥æ˜¯å¦æŒ‡å®šäº†å…·é«”æ¨¡å‹
        model_config = self.model_manager.get_model_config(request.model)
        if model_config and model_config.enabled:
            return model_config
        
        # æ ¹æ“šæ¨¡å‹åç¨±æ˜ å°„
        model_mapping = {
            "claude-3-opus": SupportedModel.CLAUDE_3_OPUS.value,
            "claude-3-sonnet": SupportedModel.CLAUDE_3_5_SONNET.value,
            "claude-3-haiku": SupportedModel.CLAUDE_3_HAIKU.value,
            "gpt-4": SupportedModel.GPT_4O.value,
            "gpt-4-turbo": SupportedModel.GPT_4_TURBO.value,
            "gpt-4o": SupportedModel.GPT_4O.value,
            "gpt-4o-mini": SupportedModel.GPT_4O_MINI.value,
            "gemini-pro": SupportedModel.GEMINI_1_5_PRO.value,
            "gemini-flash": SupportedModel.GEMINI_1_5_FLASH.value,
            "kimi-k2": SupportedModel.KIMI_K2_32K.value,
            "moonshot-v1-8k": SupportedModel.KIMI_K2_8K.value,
            "moonshot-v1-32k": SupportedModel.KIMI_K2_32K.value,
            "moonshot-v1-128k": SupportedModel.KIMI_K2_128K.value,
        }
        
        mapped_model = model_mapping.get(request.model)
        if mapped_model:
            model_config = self.model_manager.get_model_config(mapped_model)
            if model_config and model_config.enabled:
                return model_config
        
        # æ™ºèƒ½é¸æ“‡
        task_type = self._detect_task_type(request)
        max_cost = self.config.max_cost_per_request if self.config.cost_optimization else None
        
        return self.model_manager.get_best_model_for_task(task_type, max_cost)
    
    def _detect_task_type(self, request: RouterRequest) -> str:
        """æª¢æ¸¬ä»»å‹™é¡å‹"""
        # æª¢æŸ¥æ˜¯å¦åŒ…å«åœ–åƒ
        for message in request.messages:
            if isinstance(message.get("content"), list):
                for content in message["content"]:
                    if content.get("type") == "image":
                        return "vision"
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦å‡½æ•¸èª¿ç”¨
        if request.tools:
            return "function_calling"
        
        # æª¢æŸ¥ä¸Šä¸‹æ–‡é•·åº¦
        total_length = sum(len(str(msg.get("content", ""))) for msg in request.messages)
        if total_length > 50000:
            return "long_context"
        
        return "general"
    
    async def _send_request(self, request: RouterRequest, model_config: ModelConfig) -> RouterResponse:
        """ç™¼é€è«‹æ±‚åˆ°æŒ‡å®šæ¨¡å‹"""
        provider = model_config.provider
        
        if provider == ModelProvider.ANTHROPIC:
            return await self._send_anthropic_request(request, model_config)
        elif provider == ModelProvider.OPENAI:
            return await self._send_openai_request(request, model_config)
        elif provider == ModelProvider.GOOGLE:
            return await self._send_google_request(request, model_config)
        elif provider == ModelProvider.MOONSHOT:
            return await self._send_moonshot_request(request, model_config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")
    
    async def _send_anthropic_request(self, request: RouterRequest, model_config: ModelConfig) -> RouterResponse:
        """ç™¼é€Anthropicè«‹æ±‚"""
        url = f"{model_config.api_base}/v1/messages"
        headers = {
            "Content-Type": "application/json",
            "x-api-key": model_config.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        payload = request.to_anthropic_format()
        payload["model"] = model_config.model_id
        
        response = await self.http_client.post(url, json=payload, headers=headers)
        response.raise_for_status()
        
        result = response.json()
        return RouterResponse.from_anthropic_response(result, model_config.model_id, ModelProvider.ANTHROPIC)
    
    async def _send_openai_request(self, request: RouterRequest, model_config: ModelConfig) -> RouterResponse:
        """ç™¼é€OpenAIè«‹æ±‚"""
        url = f"{model_config.api_base}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {model_config.api_key}"
        }
        
        payload = request.to_openai_format()
        payload["model"] = model_config.model_id
        
        response = await self.http_client.post(url, json=payload, headers=headers)
        response.raise_for_status()
        
        result = response.json()
        return RouterResponse.from_openai_response(result, ModelProvider.OPENAI)
    
    async def _send_google_request(self, request: RouterRequest, model_config: ModelConfig) -> RouterResponse:
        """ç™¼é€Googleè«‹æ±‚"""
        url = f"{model_config.api_base}/models/{model_config.model_id}:generateContent"
        headers = {
            "Content-Type": "application/json"
        }
        
        params = {"key": model_config.api_key}
        payload = request.to_google_format()
        
        response = await self.http_client.post(url, json=payload, headers=headers, params=params)
        response.raise_for_status()
        
        result = response.json()
        return RouterResponse.from_google_response(result, model_config.model_id, ModelProvider.GOOGLE)
    
    async def _send_moonshot_request(self, request: RouterRequest, model_config: ModelConfig) -> RouterResponse:
        """ç™¼é€Moonshotè«‹æ±‚"""
        url = f"{model_config.api_base}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {model_config.api_key}"
        }
        
        payload = request.to_moonshot_format()
        payload["model"] = model_config.model_id
        
        response = await self.http_client.post(url, json=payload, headers=headers)
        response.raise_for_status()
        
        result = response.json()
        return RouterResponse.from_moonshot_response(result, ModelProvider.MOONSHOT)
    
    async def _get_cached_response(self, request: RouterRequest) -> Optional[RouterResponse]:
        """ç²å–ç·©å­˜çš„éŸ¿æ‡‰"""
        cache_key = self._generate_cache_key(request)
        cached_data = await self.cache.get(cache_key)
        
        if cached_data:
            return RouterResponse(**cached_data)
        
        return None
    
    async def _cache_response(self, request: RouterRequest, response: RouterResponse):
        """ç·©å­˜éŸ¿æ‡‰"""
        cache_key = self._generate_cache_key(request)
        await self.cache.set(cache_key, asdict(response))
    
    def _generate_cache_key(self, request: RouterRequest) -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        # å‰µå»ºè«‹æ±‚çš„å”¯ä¸€æ¨™è­˜
        request_data = {
            "model": request.model,
            "messages": request.messages,
            "temperature": request.temperature,
            "max_tokens": request.max_tokens,
            "top_p": request.top_p
        }
        
        request_str = json.dumps(request_data, sort_keys=True)
        return hashlib.md5(request_str.encode()).hexdigest()
    
    def _calculate_cost(self, response: RouterResponse, model_config: ModelConfig) -> float:
        """è¨ˆç®—è«‹æ±‚æˆæœ¬"""
        usage = response.usage
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        
        # é€šå¸¸è¼¸å‡ºtokenæˆæœ¬æ›´é«˜
        input_cost = (prompt_tokens / 1000) * model_config.cost_per_1k_tokens
        output_cost = (completion_tokens / 1000) * model_config.cost_per_1k_tokens * 3  # è¼¸å‡ºtokené€šå¸¸3å€æˆæœ¬
        
        return input_cost + output_cost
    
    async def _check_rate_limit(self, model_id: str) -> bool:
        """æª¢æŸ¥é€Ÿç‡é™åˆ¶"""
        now = time.time()
        
        if model_id not in self.request_counts:
            self.request_counts[model_id] = []
        
        # æ¸…ç†èˆŠçš„è¨˜éŒ„
        self.request_counts[model_id] = [
            timestamp for timestamp in self.request_counts[model_id]
            if now - timestamp < 60  # ä¿ç•™æœ€è¿‘1åˆ†é˜çš„è¨˜éŒ„
        ]
        
        model_config = self.model_manager.get_model_config(model_id)
        if not model_config:
            return False
        
        # æª¢æŸ¥æ˜¯å¦è¶…éé™åˆ¶
        if len(self.request_counts[model_id]) >= model_config.rate_limit_per_minute:
            return False
        
        # æ·»åŠ ç•¶å‰è«‹æ±‚
        self.request_counts[model_id].append(now)
        return True
    
    async def _update_model_metrics(self, model_id: str, response_time: float, success: bool):
        """æ›´æ–°æ¨¡å‹æ€§èƒ½æŒ‡æ¨™"""
        model_config = self.model_manager.get_model_config(model_id)
        if not model_config:
            return
        
        # æ›´æ–°å¹³å‡éŸ¿æ‡‰æ™‚é–“
        if model_config.avg_response_time == 0:
            model_config.avg_response_time = response_time
        else:
            model_config.avg_response_time = (model_config.avg_response_time * 0.9 + response_time * 0.1)
        
        # æ›´æ–°æˆåŠŸç‡
        if success:
            model_config.success_rate = min(100.0, model_config.success_rate * 0.99 + 1.0)
        else:
            model_config.success_rate = max(0.0, model_config.success_rate * 0.99)
        
        model_config.last_used = time.time()
        
        # ä¿å­˜æ›´æ–°çš„é…ç½®
        self.model_manager.save_config()
    
    async def _health_check_loop(self):
        """å¥åº·æª¢æŸ¥å¾ªç’°"""
        while True:
            try:
                await self._perform_health_checks()
                await asyncio.sleep(self.config.health_check_interval)
            except Exception as e:
                logger.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
                await asyncio.sleep(30)  # å¤±æ•—æ™‚çŸ­æš«ç­‰å¾…
    
    async def _perform_health_checks(self):
        """åŸ·è¡Œå¥åº·æª¢æŸ¥"""
        enabled_models = self.model_manager.get_enabled_models()
        
        for model_config in enabled_models:
            try:
                # å‰µå»ºç°¡å–®çš„å¥åº·æª¢æŸ¥è«‹æ±‚
                health_request = RouterRequest(
                    model=model_config.model_id,
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=5
                )
                
                start_time = time.time()
                await self._send_request(health_request, model_config)
                response_time = time.time() - start_time
                
                self.health_status[model_config.model_id] = {
                    "status": "healthy",
                    "response_time": response_time,
                    "last_check": datetime.now().isoformat()
                }
                
            except Exception as e:
                self.health_status[model_config.model_id] = {
                    "status": "unhealthy",
                    "error": str(e),
                    "last_check": datetime.now().isoformat()
                }
                
                # æš«æ™‚ç¦ç”¨ä¸å¥åº·çš„æ¨¡å‹
                model_config.enabled = False
                logger.warning(f"æ¨¡å‹ {model_config.model_id} å¥åº·æª¢æŸ¥å¤±æ•—ï¼Œå·²æš«æ™‚ç¦ç”¨")
    
    async def get_available_models(self) -> List[Dict[str, Any]]:
        """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        enabled_models = self.model_manager.get_enabled_models()
        
        return [
            {
                "model_id": model.model_id,
                "provider": model.provider.value,
                "supports_vision": model.supports_vision,
                "supports_function_calling": model.supports_function_calling,
                "context_window": model.context_window,
                "cost_per_1k_tokens": model.cost_per_1k_tokens,
                "priority": model.priority,
                "success_rate": model.success_rate,
                "avg_response_time": model.avg_response_time,
                "health_status": self.health_status.get(model.model_id, {"status": "unknown"})
            }
            for model in enabled_models
        ]
    
    async def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        return {
            "total_requests": self.stats.total_requests,
            "successful_requests": self.stats.successful_requests,
            "failed_requests": self.stats.failed_requests,
            "cached_requests": self.stats.cached_requests,
            "success_rate": self.stats.get_success_rate(),
            "cache_hit_rate": self.stats.get_cache_hit_rate(),
            "total_cost": self.stats.total_cost,
            "avg_response_time": self.stats.avg_response_time,
            "model_stats": self.stats.model_stats,
            "provider_stats": self.stats.provider_stats
        }
    
    async def switch_model(self, from_model: str, to_model: str) -> bool:
        """åˆ‡æ›æ¨¡å‹"""
        to_config = self.model_manager.get_model_config(to_model)
        if not to_config:
            logger.error(f"ç›®æ¨™æ¨¡å‹ä¸å­˜åœ¨: {to_model}")
            return False
        
        if not to_config.enabled:
            logger.error(f"ç›®æ¨™æ¨¡å‹æœªå•Ÿç”¨: {to_model}")
            return False
        
        # é€™è£¡å¯ä»¥æ·»åŠ æ›´å¤šçš„åˆ‡æ›é‚è¼¯
        logger.info(f"æ¨¡å‹åˆ‡æ›: {from_model} -> {to_model}")
        return True
    
    def get_status(self) -> Dict[str, Any]:
        """ç²å–çµ„ä»¶ç‹€æ…‹"""
        config_summary = self.model_manager.get_config_summary()
        
        return {
            "component": "Claude Code Router MCP",
            "version": "4.6.9.4",
            "status": "running",
            "uptime": time.time() - (getattr(self, '_start_time', time.time())),
            "configuration": {
                "total_models": config_summary["total_models"],
                "enabled_models": config_summary["enabled_models"],
                "cache_enabled": self.config.enable_cache,
                "load_balancing": self.config.load_balancing_strategy,
                "health_check_enabled": self.config.enable_failover
            },
            "statistics": {
                "total_requests": self.stats.total_requests,
                "success_rate": self.stats.get_success_rate(),
                "cache_hit_rate": self.stats.get_cache_hit_rate(),
                "total_cost": self.stats.total_cost
            },
            "health_status": self.health_status,
            "capabilities": [
                "multi_model_routing",
                "intelligent_load_balancing",
                "request_caching",
                "cost_optimization",
                "health_monitoring",
                "rate_limiting",
                "failover_support"
            ],
            "supported_providers": [
                "anthropic (Claude)",
                "openai (GPT)",
                "google (Gemini)",
                "moonshot (Kimi K2)"
            ]
        }
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.health_check_task:
            self.health_check_task.cancel()
        
        await self.http_client.aclose()
        await self.cache.close()
        
        logger.info("ğŸ§¹ Claude Code Router MCP æ¸…ç†å®Œæˆ")


# å–®ä¾‹å¯¦ä¾‹
claude_code_router_mcp = ClaudeCodeRouterMCP()